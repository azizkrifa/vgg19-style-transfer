{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d20e2d1",
      "metadata": {
        "id": "1d20e2d1"
      },
      "source": [
        "## üß† Introduction to Neural Style Transfer\n",
        "\n",
        "Neural Style Transfer is a computer vision technique that allows us to combine the **content of one image** with the **style of another** to produce a new, artistic image.\n",
        "\n",
        "In this project, we use the **VGG19** convolutional neural network pre-trained on ImageNet to extract:\n",
        "\n",
        "- **Content features** (structure and layout) from a **content image**\n",
        "- **Style features** (color, texture, and brushstrokes) from a **style image**\n",
        "\n",
        "Then, we optimize a new image to minimize a **loss function** that balances:\n",
        "\n",
        "- **Content loss**: how different the generated image is from the content image\n",
        "- **Style loss**: how different the textures and colors are from the style image\n",
        "\n",
        "This notebook implements the entire process step-by-step, using **TensorFlow** and **Keras**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìù Steps Overview:\n",
        "\n",
        "1. Load and preprocess content & style images\n",
        "2. Extract feature maps from VGG19 layers\n",
        "3. Compute Gram matrices for style\n",
        "4. Define loss functions\n",
        "5. Optimize an image starting from the content image\n",
        "6. Visualize stylized results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39be3401",
      "metadata": {
        "id": "39be3401"
      },
      "source": [
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f8a10f8",
      "metadata": {
        "id": "4f8a10f8"
      },
      "source": [
        "## üì¶ Step 1: Import Dependencies\n",
        "\n",
        "We begin by importing essential Python libraries:\n",
        "\n",
        "- **TensorFlow**: For deep learning and model operations (VGG19)\n",
        "- **NumPy**: For numerical operations on image tensors\n",
        "- **Matplotlib**: To visualize images during training\n",
        "- **PIL (Python Imaging Library)**: To load and manipulate images\n",
        "- **Warnings/OS**: To suppress unnecessary logs and keep the notebook clean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f752fb97",
      "metadata": {
        "id": "f752fb97"
      },
      "outputs": [],
      "source": [
        "# üì¶ Import Required Libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow and Python warnings for cleaner output\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF messages: 0=all, 1=ignore INFO, 2=ignore WARNING, 3=ignore ERROR\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c234e4",
      "metadata": {
        "id": "61c234e4"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## üñºÔ∏è Step 2: Load and Preprocess Images\n",
        "\n",
        "We define a utility function `load_image()` that:\n",
        "\n",
        "- Opens the image using **PIL**\n",
        "- Converts it to **RGB**\n",
        "- Resizes it to a maximum dimension (default: 300px) while maintaining the aspect ratio\n",
        "- Normalizes the pixel values to the [0, 1] range\n",
        "- Adds a batch dimension so it can be fed into the VGG19 model\n",
        "\n",
        "This ensures both the content and style images are in the correct format for processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e4d7c3f",
      "metadata": {
        "id": "6e4d7c3f"
      },
      "outputs": [],
      "source": [
        "def load_image(path_to_img, max_dim=300):\n",
        "    img = Image.open(path_to_img)                  # Open the image file\n",
        "    img = img.convert('RGB')                       # Ensure it's in RGB format\n",
        "    img.thumbnail((max_dim, max_dim))              # Resize while preserving aspect ratio\n",
        "    img = np.array(img)                            # Convert to NumPy array\n",
        "    img = img[tf.newaxis, ...] / 255.0             # Add batch dimension & normalize [0,1]\n",
        "    return tf.convert_to_tensor(img, dtype=tf.float32)  # Convert to TensorFlow tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484955ed",
      "metadata": {
        "id": "484955ed"
      },
      "outputs": [],
      "source": [
        "content_image = load_image(\"content.jpg\")\n",
        "style_image = load_image(\"style.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989c6ac9",
      "metadata": {
        "id": "989c6ac9"
      },
      "source": [
        "\n",
        "----\n",
        "\n",
        "## üîç Step 3: Load Pre-trained VGG19\n",
        "\n",
        "We load the **VGG19** convolutional neural network from Keras applications.\n",
        "\n",
        "- `include_top=False` removes the final fully connected layers, as we only need intermediate **feature maps**.\n",
        "- `weights='imagenet'` loads pre-trained weights that have learned rich feature representations from the ImageNet dataset.\n",
        "- `vgg.trainable = False` ensures the model parameters are **frozen** ‚Äî we‚Äôre only using it as a **feature extractor**, not for training.\n",
        "\n",
        "This model will provide the content and style representations for our input images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6465cdbf",
      "metadata": {
        "id": "6465cdbf",
        "outputId": "28840fc7-afbd-448c-9fd0-7ea46e096783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained VGG19 without the fully connected layers (for feature extraction)\n",
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "vgg.trainable = False  # Freeze the model weights; do not update during training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8631e8",
      "metadata": {
        "id": "1c8631e8"
      },
      "source": [
        "\n",
        "----\n",
        "## üß¨  Step 4: Select VGG19 Layers and Build Feature Extractors\n",
        "\n",
        "To extract meaningful information from our images, we use specific layers of the **VGG19** model:\n",
        "\n",
        "- **Content features**: captured from deeper layers (`block4_conv2`) that encode the structure and layout of the image.\n",
        "- **Style features**: captured from multiple layers to encode textures, brushstrokes, and colors at various levels.\n",
        "\n",
        "We then define a helper function that returns a new model that outputs activations from only the selected layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f20d3a50",
      "metadata": {
        "id": "f20d3a50"
      },
      "outputs": [],
      "source": [
        "content_layers = ['block4_conv2']  # Layer used to extract content features (high-level image structure)\n",
        "\n",
        "style_layers = [   # Layers used to extract style features (textures, colors, patterns)\n",
        "    'block1_conv1',\n",
        "    'block2_conv1',\n",
        "    'block3_conv1',\n",
        "    'block4_conv1',\n",
        "    'block5_conv1']\n",
        "\n",
        "\n",
        "def vgg_layers(layer_names):\n",
        "    outputs = [vgg.get_layer(name).output for name in layer_names]  # Get outputs from specified VGG layers\n",
        "    model = tf.keras.Model([vgg.input], outputs)                   # Create new model returning these outputs\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_extractor = vgg_layers(style_layers)    # Model that outputs style layer activations\n",
        "content_extractor = vgg_layers(content_layers)  # Model that outputs content layer activations"
      ],
      "metadata": {
        "id": "dkHoJR7UgstL"
      },
      "id": "dkHoJR7UgstL",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41841810",
      "metadata": {
        "id": "41841810"
      },
      "source": [
        "\n",
        "----\n",
        "\n",
        "## üß† Step 5: Extract Feature Representations\n",
        "\n",
        "To capture the **style** of an image, we use the **Gram matrix** of the feature maps. It measures correlations between different filter responses and effectively encodes textures and patterns.\n",
        "\n",
        "We also define a function to extract:\n",
        "\n",
        "- **Content features** using the `content_extractor`\n",
        "- **Style features**, converted into **Gram matrices**, using the `style_extractor`\n",
        "\n",
        "These will be used to compute the content and style loss during optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f01354a1",
      "metadata": {
        "id": "f01354a1"
      },
      "outputs": [],
      "source": [
        "# Compute the Gram Matrix (Style Representation)\n",
        "def gram_matrix(input_tensor):\n",
        "    \"\"\"\n",
        "    Computes the Gram matrix of a 4D input tensor.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (tf.Tensor): Tensor of shape (1, height, width, channels)\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Gram matrix of shape (channels, channels)\n",
        "    \"\"\"\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)  # Batch-wise dot product\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
        "    return result / num_locations  # Normalize by number of spatial locations\n",
        "\n",
        "\n",
        "\n",
        "# Extract Content and Style Features from Given Images\n",
        "def get_feature_representations(model, content_img, style_img):\n",
        "    \"\"\"\n",
        "    Extracts content and style features from the given images.\n",
        "\n",
        "    Args:\n",
        "        model: VGG19 model\n",
        "        content_img (tf.Tensor): Preprocessed content image\n",
        "        style_img (tf.Tensor): Preprocessed style image\n",
        "\n",
        "    Returns:\n",
        "        tuple: (content_features, style_gram_matrices)\n",
        "    \"\"\"\n",
        "    content_outputs = content_extractor(content_img)\n",
        "    style_outputs = style_extractor(style_img)\n",
        "    style_grams = [gram_matrix(output) for output in style_outputs]\n",
        "\n",
        "    return content_outputs, style_grams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76ed244",
      "metadata": {
        "id": "c76ed244"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## ‚öñÔ∏è Step 6: Compute Total Loss\n",
        "\n",
        "The total loss is a combination of:\n",
        "\n",
        "- **Content Loss**: Measures how different the generated image is from the content image in terms of high-level structure.\n",
        "- **Style Loss**: Measures how different the style (Gram matrices) of the generated image is from the target style image.\n",
        "\n",
        "We also allow for **per-layer weights** to control how much each style layer contributes to the overall loss.\n",
        "\n",
        "Finally, the total loss is calculated as:  \n",
        "\n",
        "**Total Loss = (style_weight √ó style_loss) + (content_weight √ó content_loss)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "952706ad",
      "metadata": {
        "id": "952706ad"
      },
      "outputs": [],
      "source": [
        "# ‚öñÔ∏è Compute Total Loss: Style + Content\n",
        "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features, style_layers, style_layer_weights):\n",
        "    \"\"\"\n",
        "    Computes the total loss combining style and content loss.\n",
        "\n",
        "    Args:\n",
        "        model: (unused) Placeholder for consistency\n",
        "        loss_weights (tuple): (style_weight, content_weight)\n",
        "        init_image (tf.Variable): Current image being optimized\n",
        "        gram_style_features (list): Target style Gram matrices\n",
        "        content_features (list): Target content features\n",
        "        style_layers (list): Names of style layers\n",
        "        style_layer_weights (dict): Layer-wise weight for style loss\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Scalar loss value\n",
        "    \"\"\"\n",
        "    input_tensor = tf.concat([init_image], axis=0)\n",
        "\n",
        "    # Get activations\n",
        "    style_output = style_extractor(input_tensor)\n",
        "    content_output = content_extractor(input_tensor)\n",
        "\n",
        "    style_score = 0\n",
        "    content_score = 0\n",
        "\n",
        "    style_weight, content_weight = loss_weights\n",
        "\n",
        "    # Compute style loss with per-layer weights\n",
        "    for i, (output, target) in enumerate(zip(style_output, gram_style_features)):\n",
        "        gram_out = gram_matrix(output)\n",
        "        layer_name = style_layers[i]\n",
        "        weight = style_layer_weights.get(layer_name, 1.0)  # Default to 1.0\n",
        "        style_score += weight * tf.reduce_mean(tf.square(gram_out - target))\n",
        "\n",
        "    # Normalize style loss (optional but improves stability)\n",
        "    total_style_weight = tf.reduce_sum(list(style_layer_weights.values()))\n",
        "    style_score /= total_style_weight\n",
        "\n",
        "    # Compute content loss\n",
        "    for output, target in zip(content_output, content_features):\n",
        "        content_score += tf.reduce_mean(tf.square(output - target))\n",
        "\n",
        "    # Total loss\n",
        "    loss = style_weight * style_score + content_weight * content_score\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e90c626",
      "metadata": {
        "id": "1e90c626"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## üöÄ Step 7: Optimize the Image\n",
        "\n",
        "We use **gradient descent** to update the pixels of the generated image so that:\n",
        "\n",
        "- It becomes more similar to the **content image** in structure.\n",
        "- It mimics the **style image** in texture and color.\n",
        "\n",
        "The `train_step()` function:\n",
        "\n",
        "1. Computes the loss using our custom loss function.\n",
        "2. Calculates gradients of the loss with respect to the image.\n",
        "3. Applies those gradients using the **Adam optimizer**.\n",
        "4. Clips the image values to keep them in the valid range \\([0, 1]\\).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6804c3cb",
      "metadata": {
        "id": "6804c3cb"
      },
      "outputs": [],
      "source": [
        "def train_step(image, loss_weights, gram_style_features, content_features, optimizer,\n",
        "               style_layers, style_layer_weights):\n",
        "    \"\"\"\n",
        "    Performs one optimization step on the image.\n",
        "\n",
        "    Args:\n",
        "        image (tf.Variable): Image being optimized\n",
        "        loss_weights (tuple): (style_weight, content_weight)\n",
        "        gram_style_features (list): Target style representations\n",
        "        content_features (list): Target content representations\n",
        "        optimizer (tf.optimizers.Optimizer): Optimizer instance\n",
        "        style_layers (list): Names of style layers\n",
        "        style_layer_weights (dict): Weights for each style layer\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(None, loss_weights, image,\n",
        "                            gram_style_features, content_features,\n",
        "                            style_layers, style_layer_weights)\n",
        "\n",
        "    # Compute gradients of the loss w.r.t. the image\n",
        "    grad = tape.gradient(loss, image)\n",
        "\n",
        "    # Apply the gradients to the image\n",
        "    optimizer.apply_gradients([(grad, image)])\n",
        "\n",
        "    # Ensure pixel values stay in valid range [0, 1]\n",
        "    image.assign(tf.clip_by_value(image, 0.0, 1.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4543cc71",
      "metadata": {
        "id": "4543cc71"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## üß™ Step 8: Stylization and Training Loop\n",
        "\n",
        "We now run the optimization process over several epochs. At each step:\n",
        "\n",
        "1. We compute the style and content loss.\n",
        "2. Update the image using the Adam optimizer.\n",
        "3. Display the stylized image at the end of each epoch.\n",
        "\n",
        "We also use:\n",
        "- **Layer weights** to control the contribution of each style layer.\n",
        "- `style_weight` and `content_weight` to balance the two objectives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a89269d3",
      "metadata": {
        "id": "a89269d3"
      },
      "outputs": [],
      "source": [
        "def show_image(tensor, title=''):\n",
        "    img = tensor.numpy().squeeze()\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222cd189",
      "metadata": {
        "id": "222cd189"
      },
      "outputs": [],
      "source": [
        "import time  # To measure training time\n",
        "\n",
        "# Style layer weights (you can tune these for different effects)\n",
        "style_layer_weights = {\n",
        "    'block1_conv1': 1,\n",
        "    'block2_conv1': 0.75,\n",
        "    'block3_conv1': 0.2,\n",
        "    'block4_conv1': 0.2,\n",
        "    'block5_conv1': 0.2\n",
        "}\n",
        "\n",
        "# Style vs Content balance\n",
        "style_weight = 1e3\n",
        "content_weight = 1\n",
        "\n",
        "# Initialize image (starting from content image)\n",
        "init_image = tf.Variable(content_image)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.003)\n",
        "\n",
        "# Extract fixed features from content and style images\n",
        "content_features, gram_style_features = get_feature_representations (vgg, content_image, style_image)\n",
        "\n",
        "# Training settings\n",
        "epochs = 10\n",
        "steps_per_epoch = 200\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Stylization loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    start_epoch = time.time()\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "        train_step(\n",
        "            init_image,\n",
        "            (style_weight, content_weight),\n",
        "            gram_style_features,\n",
        "            content_features,\n",
        "            optimizer,\n",
        "            style_layers,\n",
        "            style_layer_weights\n",
        "        )\n",
        "\n",
        "    end_epoch = time.time()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} completed in {end_epoch - start_epoch:.1f} seconds.\")\n",
        "\n",
        "    show_image(init_image, f\"Stylized Image {epoch + 1}\")\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"‚úÖ Total stylization time: {end - start:.1f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}